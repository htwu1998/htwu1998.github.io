{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd377e0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Customized Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05ff70",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### clean_product_table function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b10288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.782858Z",
     "start_time": "2021-05-11T18:30:58.775419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_product_table(product): # input: pd.DataFrame  output -> pd.DataFrame\n",
    "    \n",
    "    # remove irrelevant fields\n",
    "    product = product.drop(columns=['created_at', 'brand_canonical_url', 'product_active'])\n",
    "    \n",
    "    # concat information in different groups\n",
    "    product['product_info'] = product['name'].fillna('').astype('str') + ' ' + \\\n",
    "                            product['details'].fillna('').astype('str') + ' ' + \\\n",
    "                            product['description'].fillna('').astype('str')\n",
    "\n",
    "    product['raw_product_info'] = product['brand_name'].fillna('').astype('str') + ' ' + \\\n",
    "                                    product['brand_category'].fillna('').astype('str') + ' ' + \\\n",
    "                                    product['brand_description'].fillna('').astype('str')\n",
    "\n",
    "    product['all_info'] = product['product_info'] + ' ' + product['raw_product_info']\n",
    "    \n",
    "    product['all_info'] = product['product_info'] + ' ' + product['raw_product_info']\n",
    "    \n",
    "    # create a function to remove punctuations\n",
    "    def remove_punctuations(text):\n",
    "        ''' remove unnecessary punctuations in all lines '''\n",
    "        punctuations = ['\\n',',','.','!','\"','*','(',')','-','â€”','\\\\','@',\n",
    "                        '#','/','\\xa0',':','_','>','<',';','|','&','?','^']\n",
    "        for p in punctuations:\n",
    "            text = text.replace(p,' ')    \n",
    "        return text\n",
    "\n",
    "    # remove punctuations\n",
    "    col_list = ['product_info', 'raw_product_info', 'all_info']\n",
    "    \n",
    "    for col in col_list:\n",
    "        product[col] = product[col].apply(remove_punctuations)\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f5082",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### spacy_lemma_tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21725a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.788843Z",
     "start_time": "2021-05-11T18:30:58.784619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spacy_lemma_tokenize(product): # input: pd.DataFrame  output -> pd.DataFrame\n",
    "    \n",
    "    # load spacy library\n",
    "    import spacy\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    # clean the product_info column\n",
    "    product['clean_product_info'] = product['product_info']\\\n",
    "            .apply(lambda doc: \" \".join([token.lemma_ for token in nlp(doc) if not token.is_stop]))\n",
    "    \n",
    "    # clean the raw_product_info column\n",
    "    product['clean_raw_product_info'] = product['raw_product_info']\\\n",
    "            .apply(lambda doc: \" \".join([token.lemma_ for token in nlp(doc) if not token.is_stop]))\n",
    "    \n",
    "    # clean the all_info column\n",
    "    product['clean_all_info'] = product['all_info']\\\n",
    "            .apply(lambda doc: \" \".join([token.lemma_ for token in nlp(doc) if not token.is_stop]))\n",
    "\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30efcb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_clothing_category_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74a3b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.807698Z",
     "start_time": "2021-05-11T18:30:58.790930Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_clothing_category_feature(product, outfit): \n",
    "# input:pd.DataFrame  output -> pd.Series, pd.DataFrame, pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from collections import Counter\n",
    "    \n",
    "    def unfold_regex(category_find):\n",
    "        new = []\n",
    "        for i in category_find:\n",
    "            for j in i:\n",
    "                if j != '':\n",
    "                    new.append(j)\n",
    "        return new\n",
    "    \n",
    "    def find_mode_category(product_info):\n",
    "\n",
    "        top_regex = r'(?i)\\b(t(?:-)?shirt)s?\\b|\\b(shirt)s?\\b|\\b(blouse)s?\\b|\\b(tank)s?\\b|\\b(top)s?\\b|'\\\n",
    "                        r'\\b(suit)s?\\b|\\b(sweatshirt)s?\\b|\\b(knitwear)s?\\b|\\b(vest)s?\\b|\\b(suit)s?\\b|\\b(sweater)s?\\b|'\\\n",
    "                        r'\\b(cardigan)\\b|\\b(tee)s?\\b|\\b(hoodie)s?\\b|\\b(camisole)s?\\b|\\b(turtleneck)s?\\b'\n",
    "        shoe_regex = r'(?i)\\b(shoe)s?\\b|\\b(sneaker)s?\\b|\\b(footie)s?\\b|\\b(footwear)s?\\b|\\b(pump)s?\\b|'\\\n",
    "                        r'\\b(flat)s?\\b|\\b(heel)s?\\b|\\b(boot)s?\\b|\\b(bootie)s?\\b|\\b(loafer)s?\\b'\\\n",
    "                        r'|\\b(mule)s?\\b|\\b(sandal)s?\\b|\\b(slipper)s?\\b|\\b(wedge)s?\\b|\\b(slide)s?\\b|\\b(slingback)s?\\b'\n",
    "        bottom_regex = r'(?i)\\b(jean)s?\\b|\\b(short)s?\\b|\\b(pant)s?\\b|\\b(skirt)s?\\b|\\b(skort)s?\\b|\\b(sweatpant)s?\\b|'\\\n",
    "                        r'\\b(legging)s?\\b|\\b(trouser)s?\\b|\\b(bottom)s?\\b|\\b(jogger)s?\\b|\\b(tight)s?\\b|\\b(crop)s?\\b|\\b(leg)s?\\b'\n",
    "        bag_regex = r'(?i)\\b(bag)s?\\b|\\b(handbag)s?\\b|\\b(shoulderbag)s?\\b|\\b(tote)s?\\b|\\b(clutch)(?:es)?\\b|'\\\n",
    "                        r'\\b(luggage)s?\\b|\\b(belt\\s?bag)s?\\b|\\b(beach\\s?bag)s?\\b|\\b(backpack)s?\\b|\\b(satchel)s?\\b|'\\\n",
    "                        r'\\b(briefcase)s?\\b|\\b(pouch)(?:es)?\\b'\n",
    "        accessory_regex = r'(?i)\\b(scarf|scarves)\\b|\\b(hat)s?\\b|\\b(belt)s?\\b|\\b(sunglass)es?\\b|\\b(glove)s?\\b|'\\\n",
    "                            r'\\b(keychain)s?\\b|\\b(keyring)s?\\b|\\b(tie)s?\\b|\\b(phone\\s?case)s?\\b|'\\\n",
    "                            r'\\b(glass)(?:es)?\\b|\\b(umbrella)s?\\b|\\b(frame)s?\\b|\\b(wallet)s?\\b|\\b(face\\s?mask)s?\\b|'\\\n",
    "                            r'\\b(helmet)s?\\b|\\b(shawl)s?\\b'\n",
    "        onepiece_regex = r'(?i)\\b(dress)(?:es)?\\b|\\b(jumpsuit)s?\\b|\\b(gown)s?\\b|\\b(robe)s?\\b|\\b(shirtdress)(?:es)?\\b|\\b(bodysuit)s?\\b'\n",
    "        outerwear_regex= r'(?i)\\b(?:top)?(coat)s?\\b|\\b(jacket)s?\\b|\\b(parka)s?\\b|\\b(trench)(?:es)?\\b|\\b(raincoat)s?\\b|\\b(overcoat)s?\\b|'\\\n",
    "                            r'\\b(blazer)s?\\b'\n",
    "        jewelry_regex = r'(?i)\\b(bracelet)s?\\b|\\b(brooch)(?:es)?\\b|\\b(pin)s?\\b|\\b(cufflink)s?\\b|\\b(earring)s?\\b|'\\\n",
    "                            r'\\b(necklace)s?\\b|\\b(ring)s?\\b'\n",
    "        intimate_regex = r'(?i)\\b(underwear)s?\\b|\\b(bra)s?\\b|\\b(sock)s?\\b|\\b(sleepwear)s?\\b|\\b(loungewear)s?\\b|'\\\n",
    "                            r'\\b(boxer)s?\\b|\\b(brief)s?\\b|\\b(linger)(?:ies)?\\b|\\b(pantie)s?\\b'\n",
    "\n",
    "        top_find = re.findall(top_regex, product_info)\n",
    "        shoe_find = re.findall(shoe_regex, product_info)\n",
    "        bottom_find = re.findall(bottom_regex, product_info)\n",
    "        bag_find = re.findall(bag_regex, product_info)\n",
    "        accessory_find = re.findall(accessory_regex, product_info)\n",
    "        onepiece_find = re.findall(onepiece_regex, product_info)\n",
    "        outerwear_find = re.findall(outerwear_regex, product_info)\n",
    "        jewelry_find = re.findall(jewelry_regex, product_info)\n",
    "        intimate_find = re.findall(intimate_regex, product_info)\n",
    "\n",
    "        top_find = unfold_regex(top_find)\n",
    "        shoe_find = unfold_regex(shoe_find)\n",
    "        bottom_find = unfold_regex(bottom_find)\n",
    "        bag_find = unfold_regex(bag_find)\n",
    "        accessory_find = unfold_regex(accessory_find)\n",
    "        onepiece_find = unfold_regex(onepiece_find)\n",
    "        outerwear_find = unfold_regex(outerwear_find)\n",
    "        jewelry_find = unfold_regex(jewelry_find)\n",
    "        intimate_find = unfold_regex(intimate_find)\n",
    "\n",
    "        category_word_count_dict = {'top':len(top_find), 'bottom':len(bottom_find), 'shoe':len(shoe_find),\\\n",
    "                                            'bag':len(bag_find),'accessory':len(accessory_find),\\\n",
    "                                            'onepiece':len(onepiece_find),'outerwear':len(outerwear_find),\\\n",
    "                                            'jewelry':len(jewelry_find),'intimate':len(intimate_find)}\n",
    "        mode_category_name = ''\n",
    "        if sum(category_word_count_dict.values()) != 0:\n",
    "            mode_category_name = max(category_word_count_dict, key=category_word_count_dict.get)\n",
    "\n",
    "\n",
    "        return mode_category_name\n",
    "\n",
    "    def find_sub_category(product_info):\n",
    "\n",
    "        all_category_regex = r'(?i)\\b(t(?:-)?shirt)s?\\b|\\b(shirt)s?\\b|\\b(blouse)s?\\b|\\b(tank)s?\\b|'\\\n",
    "                            r'\\b(suit)s?\\b|\\b(sweatshirt)s?\\b|\\b(knitwear)s?\\b|\\b(vest)s?\\b|\\b(suit)s?\\b|'\\\n",
    "                            r'\\b(sneaker)s?\\b|\\b(footie)s?\\b|\\b(footwear)s?\\b|\\b(pump)s?\\b|'\\\n",
    "                            r'\\b(flat)s?\\b|\\b(heel)s?\\b|\\b(boot)s?\\b|\\b(bootie)s?\\b|\\b(loafer)s?\\b|'\\\n",
    "                            r'\\b(mule)s?\\b|\\b(sandal)s?\\b|\\b(slipper)s?\\b|\\b(wedge)s?\\b|'\\\n",
    "                            r'\\b(jean)s?\\b|\\b(short)s?\\b|\\b(pant)s?\\b|\\b(skirt)s?\\b|\\b(skort)s?\\b|'\\\n",
    "                            r'\\b(legging)s?\\b|\\b(trouser)s?\\b|\\b(jogger)s?\\b|\\b(sweater)s?\\b'\\\n",
    "                            r'\\b(bag)s?\\b|\\b(handbag)s?\\b|\\b(shoulderbag)s?\\b|\\b(tote)s?\\b|\\b(clutch)(?:es)?\\b|'\\\n",
    "                            r'\\b(luggage)s?\\b|\\b(belt\\s?bag)s?\\b|\\b(beach\\s?bag)s?\\b|\\b(backpack)s?\\b|'\\\n",
    "                            r'\\b(scarf|scarves)\\b|\\b(hat)s?\\b|\\b(belt)s?\\b|\\b(sunglass)(?:es)?\\b|\\b(glove)s?\\b|'\\\n",
    "                            r'\\b(keychain)s?\\b|\\b(keyring)s?\\b|\\b(tie)s?\\b|\\b(phone\\s?case)s?\\b|'\\\n",
    "                            r'\\b(glass)(?:es)?\\b|\\b(umbrella)s?\\b|\\b(frame)s?\\b|\\b(wallet)s?\\b|\\b(face\\s?mask)s?\\b|'\\\n",
    "                            r'\\b(helmet)s?\\b|\\b(dress)(?:es)?\\b|\\b(jumpsuit)s?\\b|\\b(gown)s?\\b|\\b(robe)s?\\b|'\\\n",
    "                            r'\\b(coat)s?\\b|\\b(jacket)s?\\b|\\b(parka)s?\\b|\\b(trench)(?:es)?\\b|\\b(raincoat)s?\\b|'\\\n",
    "                            r'\\b(overcoat)s?\\b|\\b(bracelet)s?\\b|\\b(brooch)(?:es)?\\b|\\b(pin)s?\\b|\\b(cufflink)s?\\b|'\\\n",
    "                            r'\\b(earring)s?\\b|\\b(necklace)s?\\b|\\b(ring)s?\\b|\\b(underwear)s?\\b|\\b(bra)s?\\b|'\\\n",
    "                            r'\\b(sock)s?\\b|\\b(sleepwear)s?\\b|\\b(loungewear)s?\\b|\\b(boxer)s?\\b|\\b(brief)s?\\b|'\\\n",
    "                            r'\\b(linger)(?:ies)?\\b|\\b(pantie)s?\\b|\\b(satchel)s?\\b|\\b(sweater)s?\\b|\\b(tee)s?\\b|'\\\n",
    "                            r'\\b(tight)s?\\b|\\b(cardigan)s?\\b|\\b(hoodie)s?\\b|\\b(sweatpant)s?\\b|\\b(slide)s?\\b|'\\\n",
    "                            r'\\b(shirtdress)(?:es)?\\b|\\b(blazer)s?\\b|\\b(crop)s?\\b|\\b(leg)s?\\b|\\b(briefcase)s?\\b|'\\\n",
    "                            r'\\b(shawl)s?\\b|\\b(camisole)s?\\b|\\b(bodysuit)s?\\b|\\b(turtleneck)s?\\b|\\b(pouch)(?:es)?\\b|' \\\n",
    "                            r'\\b(slingback)s?\\b'\n",
    "\n",
    "        all_category_find = re.findall(all_category_regex, product_info)\n",
    "        all_category_find = unfold_regex(all_category_find)\n",
    "        word_counter = Counter(all_category_find)\n",
    "        sub_category = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "\n",
    "        if len(word_counter) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return sub_category[0]\n",
    "        \n",
    "    df = product.copy()\n",
    "    \n",
    "    df['mode_category']=df['all_info'].apply(find_mode_category)\n",
    "    df['sub_category'] = df['all_info'].apply(find_sub_category)\n",
    "    \n",
    "    temp = outfit[['product_id','outfit_item_type']].drop_duplicates()\n",
    "    temp['outfit_item_type'] = temp['outfit_item_type'].str.replace('accessory1','accessory')\n",
    "    temp['outfit_item_type'] = temp['outfit_item_type'].str.replace('accessory2','accessory')\n",
    "    temp['outfit_item_type'] = temp['outfit_item_type'].str.replace('accessory3','accessory')\n",
    "    temp1 = temp.groupby('product_id').agg({'outfit_item_type':'count'}).reset_index()\n",
    "    productid_only_1 = temp1.loc[temp1['outfit_item_type']==1,['product_id']]\n",
    "    new_outfit = productid_only_1.merge(temp, how='inner',on='product_id')\n",
    "\n",
    "    # merge product df with new_outfit df\n",
    "    merged = df.merge(new_outfit, how='left', on='product_id')\n",
    "\n",
    "    ind = merged.loc[merged['outfit_item_type'].notnull()==True,].index\n",
    "\n",
    "    for i in ind:\n",
    "        merged.loc[i,'mode_category'] = merged.loc[i,'outfit_item_type']\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    X_mode = vectorizer.fit_transform(merged['mode_category'])\n",
    "    vectorized_df_mode = pd.DataFrame(X_mode.toarray(), columns=vectorizer.get_feature_names())\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    X_sub = vectorizer.fit_transform(merged['sub_category'])\n",
    "    vectorized_df_sub = pd.DataFrame(X_sub.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return merged['mode_category'], vectorized_df_mode, merged['sub_category'], vectorized_df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d4a93",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_gender_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cec5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.817970Z",
     "start_time": "2021-05-11T18:30:58.809689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_gender_feature(product): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # gender regex patterns\n",
    "    female_regex = r'\\b(women|ladies|lady|woman|female|dress(?:es)?|skorts?|skirts?|blouses?|girls?|jewelr(?:ies)?|heels?)\\b'\n",
    "    male_regex = r'\\b(man|men|gentleman|gentlemen)\\b'\n",
    "    kid_regex = r'\\b(kids?|child(?:ren)?|girls?|boys?|babies|baby|infants?|toddlers?|teenagers?|youths?)\\b'\n",
    "    \n",
    "    def find_all_gender_words(product_info):\n",
    "    \n",
    "        # initiate an empty dictionary to store words count in each category\n",
    "        # initiate empty list to store results\n",
    "        female_find = re.findall(female_regex, product_info)\n",
    "        male_find = re.findall(male_regex, product_info)\n",
    "        kid_find = re.findall(kid_regex, product_info)\n",
    "\n",
    "        gender_list = female_find+male_find+kid_find\n",
    "        gender_list = [i for i in gender_list if i != \"\"]\n",
    "        clean_gender_results = \" \".join([i for i in gender_list])\n",
    "\n",
    "        return clean_gender_results\n",
    "\n",
    "    def find_mode_gender(product_info):\n",
    "    \n",
    "        gender_word_count_dict = {}\n",
    "        female_find = re.findall(female_regex, product_info)\n",
    "        female_find = [i for i in female_find if i != \"\"]\n",
    "        male_find = re.findall(male_regex, product_info)\n",
    "        male_find = [i for i in male_find if i != \"\"]\n",
    "        kid_find = re.findall(kid_regex, product_info)\n",
    "        kid_find = [i for i in kid_find if i != \"\"]\n",
    "        gender_list = female_find+male_find+kid_find\n",
    "        gender_list = [i for i in gender_list if i != \"\"]\n",
    "\n",
    "        gender_word_count_dict = {'women':len(female_find),'men':len(male_find),'kid':len(kid_find)}\n",
    "\n",
    "        if sum(gender_word_count_dict.values()) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            mode_gender = max(gender_word_count_dict, key=gender_word_count_dict.get)\n",
    "            return mode_gender\n",
    "    \n",
    "    df = product.copy()\n",
    "    df['all_gender_words'] = df['raw_product_info'].apply(find_all_gender_words)\n",
    "    df['mode_gender'] = df['raw_product_info'].apply(find_mode_gender)\n",
    "    \n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    X = vectorizer.fit_transform(df['mode_gender'])\n",
    "    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return df['mode_gender'], vectorized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce41193",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_material_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df4718d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.824565Z",
     "start_time": "2021-05-11T18:30:58.819341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_material_feature(df): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # define color pattern keywords\n",
    "    materials = ['acetate', 'acrylic', 'alpaca', 'calf', 'cashmere','chiffon', 'cotton','kidskin', \n",
    "            'lamb', 'lambskin', 'leather','linen', 'lyocell','mercerized', 'merino','nylon',\n",
    "            'organic','peruvian', 'pima','poly', 'polyamide','polyester', 'polyurethane', \n",
    "            'ramie', 'rayon','rubber', 'silk','supima', 'tencel', 'triacetate', 'uv', 'uva',\n",
    "            'velvet', 'virgin', 'viscose', 'wood', 'wool','rose sylk']\n",
    "    \n",
    "    material_pattern = r\"\\b(\"+\"|\".join(materials)+r\")\\b\"\n",
    "    \n",
    "    # find all material words from given info\n",
    "    df['material'] = df['all_info'].str.lower().str.findall(material_pattern)\n",
    "    \n",
    "    # remove all duplicates\n",
    "    df['material'] = df['material'].map(lambda x: list(set(word.lower() for word in x)))\n",
    "    \n",
    "    # convert material finding to vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    temp = df['material'].apply(lambda doc: \" \".join([word for word in doc]))\n",
    "    X = vectorizer.fit_transform(temp)\n",
    "    material_result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return df['material'],material_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b1261",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_percent_material_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea8deb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.835373Z",
     "start_time": "2021-05-11T18:30:58.825931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_percent_material_feature(df): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # package up codes for extracting p_material features from product description and details\n",
    "    def func2(row):\n",
    "        \"\"\"\n",
    "        this function is for the case where one of the \"p_material_desc\" and \"p_material_details\"\n",
    "        is null.\n",
    "        \"\"\"\n",
    "        if type(row['p_material_desc']) == list:\n",
    "            return row['p_material_desc']\n",
    "        else:\n",
    "            return row['p_material_details']\n",
    "        \n",
    "    # defined specific regex to extract percent_material from details and description\n",
    "    # examined by looking into some special cases of text to make regex more precise\n",
    "\n",
    "    p_material_regex = r'([\\d]{1,3}%\\s(?!of|for|Deadstock|dead|Hand)[\\w\\s]+?)(?=\\n|\\d|\\,|\\/|$|:|\\.|Care|\\bis\\b|Ma|Hand|Pr|grown|for|from|with|into|Category|\\bin\\b|\\band\\b|Dry|\\bto\\b|\\bAnd\\b|\\bCollection\\b)'\n",
    "\n",
    "    # look through both deescription and details columns\n",
    "    df['p_material_desc'] = df['description'].str.findall(p_material_regex)\n",
    "    df['p_material_details'] = df['details'].str.findall(p_material_regex)\n",
    "    \n",
    "    # combine features extracted from description and details using \"func2\" to avoid null value distraction\n",
    "    df['new'] = df[['p_material_desc','p_material_details']].apply(lambda x: x['p_material_desc']+x['p_material_details'] if x.notnull().all() else func2(x), axis=1)\n",
    "    \n",
    "    # replace empty list with na\n",
    "    df['new_1'] = df['new'].apply(lambda x: np.nan if type(x)==list and len(x)==0 else x)\n",
    "    # lower case, strip white space and remove duplicates\n",
    "    df['new_2'] = df['new_1'].apply(lambda x: list(set([a.lower().strip() for a in x])) if type(x)==list else x)\n",
    "\n",
    "    df.rename(columns={'new_2': 'p_material'}, inplace=True)\n",
    "    \n",
    "    # change list type value to string type\n",
    "    df['p_material'] = df['p_material'].apply(lambda x: ' '.join(x) if type(x)==list else x)\n",
    "    # fill null value with empty string\n",
    "    df['p_material'] = df['p_material'].fillna('')\n",
    "    \n",
    "    # replace remaining abnormal symbols \n",
    "    df['p_material'] = df['p_material'].str.replace(r'\\n|\\xa0',' ')\n",
    "    \n",
    "    # only count the first word after percentage symbol as p_material \n",
    "    df['p_material'] = df['p_material'].str.findall(r'[\\d]{1,3}%\\s[\\w]+').apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # using regex to keep over 60% percent_material as our final feature\n",
    "    df['p_material'] = df['p_material'].str.findall(r'[1]?[0|6-9][0-9]%\\s[\\w]+').apply(lambda x: ' '.join(x) if type(x)==list else x)\n",
    "    \n",
    "    # finally fill null value with empty string to avoid future process\n",
    "    df['p_material'] = df['p_material'].fillna('')\n",
    "    \n",
    "    # vectorize result\n",
    "    docs = df['p_material'].str.replace(r'[\\d]+%\\s','').str.split().apply(lambda x: list(set(x))).apply(lambda x: ' '.join(x)).tolist()\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=20)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return df['p_material'], data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77119212",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_sizes_features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cd8d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.843328Z",
     "start_time": "2021-05-11T18:30:58.836934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_sizes_features(data): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    #read data\n",
    "\n",
    "    #lowercase 2 columns\n",
    "    lower_description = []\n",
    "    for des in data['description']:\n",
    "        lower_description.append(str(des).lower())\n",
    "    lower_details = []\n",
    "    for det in data['details']:\n",
    "        lower_details.append(str(det).lower())\n",
    "\n",
    "    #finding size in every sentense\n",
    "    sizes_des = []\n",
    "    sizes_det = []\n",
    "    for i in lower_description:\n",
    "        sizes_des.append(str(set(re.findall(r'(?<=\\bsize )(?:\\d+|big|large|middle|medium|small|l|m|s|x{1,}s|x{1,}l|\\b)',i))))\n",
    "    for i in lower_details:\n",
    "        sizes_det.append(str(set(re.findall(r'(?<=\\bsize )(?:\\d+|big|large|middle|medium|small|l|m|s|x{1,}s|x{1,}l|\\b)',i))))\n",
    "\n",
    "    #Getting rid of puncuations \n",
    "    exp = r'(\\'|[\\{\\}]|,)|set\\(\\)'\n",
    "    new_1 = []\n",
    "    for i in sizes_des:\n",
    "        new_1.append(re.sub(exp,'',i))\n",
    "    new_2 = []\n",
    "    for i in sizes_det:\n",
    "        new_2.append(re.sub(exp,'',i))\n",
    "\n",
    "    l = []\n",
    "    for i in range(len(new_1)):\n",
    "        l.append(new_1[i] + \" \" + new_2[i])\n",
    "\n",
    "    #Transfer into dataframe\n",
    "    size = pd.DataFrame()\n",
    "    size['sizes'] = l\n",
    "\n",
    "    #Function of turning none value in to string\n",
    "    def changenull(x):\n",
    "        if len(x) == 0:\n",
    "            return \"None\"\n",
    "        else:\n",
    "            return x\n",
    "    size['sizes'] = size['sizes'].apply(changenull)\n",
    "\n",
    "    #Vectorizing with one-hot encoding\n",
    "    vectorizer = CountVectorizer(binary=True,min_df=10)\n",
    "    X = vectorizer.fit_transform(size['sizes'])\n",
    "    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return size, vectorized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7398f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_color_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f641357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.851354Z",
     "start_time": "2021-05-11T18:30:58.846726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_color_feature(df): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # define color pattern keywords\n",
    "    color_pattern = r'(?i)\\bRed|Orange|Yellow|Green|Blue|Purple|White|Black|Brown|Magenta|Tan|Olive|Navy|Turquoise|Silver|Lime|Teal|Indigo|Violet|Pink|Gray|Navy|Beige|Burgundy|Golden|Magenta|Cyan|Aquamarine\\b'\n",
    "    \n",
    "    # find all color words from given info\n",
    "    data = pd.DataFrame([i for i in range(len(df))], columns=['Document'])\n",
    "    data['color'] = df['all_info'].str.findall(color_pattern)\n",
    "    \n",
    "    # fill null value with `[]`\n",
    "    data.loc[data['color'].isnull(), 'color'] = data.loc[data['color'].isnull(), 'color'].apply(lambda x: [])\n",
    "    \n",
    "    # remove all duplicates\n",
    "    data['color'] = data['color'].map(lambda x: list(set(word.lower() for word in x)))\n",
    "    \n",
    "    # convert color finding to vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    temp = data['color'].apply(lambda doc: \" \".join([word for word in doc]))\n",
    "    X = vectorizer.fit_transform(temp)\n",
    "    color_result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return data['color'],color_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc91db",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_location_feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74cc0179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.859400Z",
     "start_time": "2021-05-11T18:30:58.853918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_location_feature(df): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # define location keywords\n",
    "    # check for false postives: us, la, case insensitive\n",
    "    # define location keywords\n",
    "    location_regex_1 = r'(?i)\\b(usa|italy|ethiopia|china|peru|los angeles|nyc|spain|new york|portugal|india|america|kenya|turkey|brazil|ghana|italian|morocco|france|vietnam|germany|lima|mexico|argentina|japan|brooklyn|madagascar|bali|prc|poland)\\b'\n",
    "    location_regex_2 = r'\\b(LA|US|JP)\\b'\n",
    "    \n",
    "    # find all location words from given info\n",
    "    data = pd.DataFrame([i for i in range(len(df))], columns=['Document'])\n",
    "    data['location_1'] = df['all_info'].str.findall(location_regex_1)\n",
    "    data['location_2'] = df['all_info'].str.findall(location_regex_2)\n",
    "    \n",
    "    # fill null value with `[]`\n",
    "    data.loc[data['location_1'].isnull(), 'location_1'] = data.loc[data['location_1'].isnull(), 'location_1'].apply(lambda x: [])\n",
    "    data.loc[data['location_2'].isnull(), 'location_2'] = data.loc[data['location_2'].isnull(), 'location_2'].apply(lambda x: [])\n",
    "    \n",
    "    # concat two findings\n",
    "    data['location'] = data['location_1'] + data['location_2']\n",
    "    \n",
    "    # remove all duplicates\n",
    "    data['location'] = data['location'].map(lambda x: list(set(word.lower() for word in x)))\n",
    "    \n",
    "    # create fields for each color\n",
    "    vectorizer = CountVectorizer()\n",
    "    temp = data['location'].apply(lambda doc: \" \".join([word for word in doc]))\n",
    "    X = vectorizer.fit_transform(temp)\n",
    "    location_result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return data['location'], location_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357bbb9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### create_tags_feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b7a3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:30:58.876017Z",
     "start_time": "2021-05-11T18:30:58.861190Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_tags_feature(df): # input:pd.DataFrame  output -> pd.Series, pd.DataFrame\n",
    "    \n",
    "    # load libraries\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    tags_words = ['stripe vertical', 'shoulder bags', 'highneck', 'romantic', 'crossbody', 'sweatshirt', \n",
    "                  'denim', 'cotton blend', 'straightregular', 'buttonedback', 'backzipper', 'multiprint', \n",
    "                  'sportsbra', 'joggerssweatpants', 'henley', 'sheath', 'dropwaist', 'mohair', 'skinny', \n",
    "                  'jerseyknit', 'short at waistline', 'cold weather', 'corduroy', 'tank', 'splitneck', 'vneck',\n",
    "                  'camisole', 'tiedye', 'shawlcollar', 'straight regular', 'shell', 'satincharmeuse', \n",
    "                  'lightbrowns', 'waist', 'roundtoe', 'bandcollar', 'sandals', 'buttondown', 'above waistline', \n",
    "                  'plus', 'purewool', 'sleeve', 'flap', 'kitten', 'businesscasualdress', 'down', 'bucketbags', \n",
    "                  'classic', 'strap', 'belted', '5pocketpantnondenim', 'boot', 'floral', 'mid length at hips', \n",
    "                  'squaretoe', 'edgy', 'tote', 'booties', 'fauxshearling', 'bucket', 'faux shearling', \n",
    "                  'lightbrown', 'straight', 'cone', 'oneshoulder', 'squareneck', 'culotte', 'beachbags', \n",
    "                  'hoodie', 'pinstripe', 'cases', 'faux leather', 'regular', 'blacks', 'zipup', 'hook', \n",
    "                  'twisted', 'fauxfur', 'crepedechine', 'sidezip', 'halfwayzipper', 'stripevertical', \n",
    "                  'suede', 'boots', 'tall', 'buttonfront', 'open', 'purecotton', 'trackpants', 'shortsleeve', \n",
    "                  'platform', 'sleeveless', 'cut', 'sweatshirthoodie', 'halfwaybuttoned', 'swraps', 'logo', \n",
    "                  'zipflywithbutton', 'pumpsheels', 'burgundies', 'twill', 'shawl collar', 'tweed', \n",
    "                  'midlengthathips', 'joggers', 'crepe de chine', 'aline', 'buckle', 'satchels', 'wedge', \n",
    "                  'capri', 'business casual', 'blouse', 'glam', 'bodycon', 'knit', 'jewel', 'purelinen', \n",
    "                  'mandarincollar', 'color block', 'tiefront', 'heels', 'fleece', 'crossbodybags', 'poncho', \n",
    "                  'wrap', 'wedges', 'duster', 'opentoe', 'vest', 'retro', 'patent leather', 'turtleneck', \n",
    "                  'short sleeve', 'light brown', 'button', 'zipfly', 'grays', 'satin charmeuse', 'multi print',\n",
    "                  'funnel', 'pantsleggings', 'wideleg', 'halter', 'pinks', 'clasp', 'women', 'silk blend', \n",
    "                  'mandarin', 'casualdress', 'velcro', 'scarveswraps', 'stripehorizontal', 'dark brown', \n",
    "                  'cropped', 'daytonight', 'canvas', 'block', 'stiletto', 'surplice', 'golds', 'geometric', \n",
    "                  'faux fur', 'empirewaist', 'coldweather', 'nightout', 'maternity', 'boyfriend', 'elastane', \n",
    "                  'narrow', 'draped', 'laceup', 'silvers', 'baggy', 'sneakersathletic', 'pussybow', \n",
    "                  'casual dress', 'animal', 'drawstring', 'casual', 'cowlneck', 'lock', 'inwardcurve', \n",
    "                  'under8', 'oranges', 'shirtdress', 'tieback', 'tropical', 'sateen', 'walletscardcases', \n",
    "                  'linen blend', 'buttoned', 'belowhips', 'multi', 'day to night', 'highover9', 'plungeneck', \n",
    "                  'tee', 'platformflatform', 'tailored', 'polo', 'fauxleather', 'bags', 'linenblend', \n",
    "                  'sweatpants', 'slingback', 'androgynous', 'purples', 'sweetheart', 'tie', 'blazerdress', \n",
    "                  'purecashmere', 'slit', 'blazerscoatsjackets', 'anklestrap', 'handbags', 'closedtoe', \n",
    "                  'strapless', 'pants', 'mules', 'skirts', 'semi fitted', 'low', 'totebags', 'snap', \n",
    "                  'fittedtailored', 'sneakers athletic', 'openfront', 'puffsleeve', 'crewneck', 'synthetic',\n",
    "                  'raisedsole', 'magnetic', 'zip', 'darkbrowns', 'hobobags', 'long', 'cardigan', 'calfhair', \n",
    "                  'beach bags', 'pointedtoe', 'maxi', 'peeptoe', 'laces', 'silkblend', 'wide', 'paisley',\n",
    "                  'shortatwaistline', 'bustier', 'shorts', 'flats', 'colorblock', 'stripe', \n",
    "                  'blazers coats jackets', 'laptopsbriefcases', 'peplum', 'beltbagsfannypack', 'patentleather',\n",
    "                  'cold', 'slipdress', 'shoulderbags', 'round toe', 'croptop', 'cargo', 'shearling', 'shoulder',\n",
    "                  'midcalf', 'snaps', 'back', 'collar', 'blues', 'leggings', 'hookandloop', 'relaxed', 'greens',\n",
    "                  'sweater', 'chambray', 'hookloop', 'tshirtdress', 'zipper', 'tieneck', 'longsleeve', \n",
    "                  'cashmereblend', 'longbelowhips', 'slim', 'weekend', 'halfway buttoned', 'oversized', \n",
    "                  'reds', 'slippers', 'sunglasses', 'modal', 'tiered', 'whites', 'belts', 'sports bra', \n",
    "                  'cottonblend', 'high', 'boho', 'stripe horizontal', 'sundress', 'backpacks', 'beiges', \n",
    "                  'scarve', 'graphic', 'fannypack', 'sweaterdress', 'flare', 'dots', 'boatneck', 'nondenim', \n",
    "                  'businesscasual', 'empire', 'woolblend', 'slides', 'laptops briefcases', 'houndstooth', \n",
    "                  'capsleeve', 'pure linen', 'flatform', 'athleisure', 'ponyhair', 'scoopneck', 'calf hair', \n",
    "                  'denimjeans', 'none', 'clutchespouches', 'front', 'coldshoulder', 'bodysuit', \n",
    "                  'zipflywithhook', 'buttonedfront', 'spandex', 'collared', 'puff', 'vacation', 'abstract', \n",
    "                  'side', 'snakeskin', 'yellows', 'monogram', 'keyhole', 'closed toe', 'designer', \n",
    "                  'long sleeve', 'backzip', 'chenille', 'asymmetrical', 'fitted', 'tunic', \n",
    "                  'croppedabovewaistline', 'active', 'cap', 'pointed', 'camouflage', 'toe', 'frontzip', \n",
    "                  'bootcut', 'backless', 'halfway zipper', 'tie dye', 'mockneck', 'offshoulder', 'shift', \n",
    "                  'trousers', 'puresilk', 'walletscard', 'workout', 'pumps', 'ankle', 'mulesslides', \n",
    "                  'semifitted', '5pocketpant', 'trapezeswing', 'mid89', 'modern', 'work', 'flat', 'plaid', \n",
    "                  'belt']\n",
    "    \n",
    "    tags_words = list(set(tags_words))\n",
    "    \n",
    "    # create tags regex\n",
    "    super_regex = r\"\\b\" + \"|\".join([words for words in tags_words]) + r\"\\b\"\n",
    "    \n",
    "    # find all tags words from given info\n",
    "    data = pd.DataFrame([i for i in range(len(df))], columns=['Document'])\n",
    "    data['tags'] = df['all_info'].str.findall(super_regex)\n",
    "    \n",
    "    # fill null value with `[]`\n",
    "    data.loc[data['tags'].isnull(), 'tags'] = data.loc[data['tags'].isnull(), 'tags'].apply(lambda x: [])\n",
    "    \n",
    "    # remove all duplicates\n",
    "    data['tags'] = data['tags'].map(lambda x: list(set(word.lower() for word in x)))\n",
    "    \n",
    "    # convert tags finding to vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    temp = data['tags'].apply(lambda doc: \" \".join([word for word in doc]))\n",
    "    X = vectorizer.fit_transform(temp)\n",
    "    tags_result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return data['tags'],tags_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863710f6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af6b52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:31:07.140716Z",
     "start_time": "2021-05-11T18:30:58.877986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "product = pd.read_excel('data/Behold+product+data+04262021.xlsx')\n",
    "brand = pd.read_csv('data/behold_brands USC.csv')\n",
    "combination = pd.read_csv('data/outfit_combinations USC.csv')\n",
    "tags = pd.read_csv('data/usc_additional_tags USC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba9ba893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:31:08.125804Z",
     "start_time": "2021-05-11T18:31:07.142109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initial cleaning\n",
    "clean_product = clean_product_table(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4b7b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:31:09.891931Z",
     "start_time": "2021-05-11T18:31:08.127699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spacy lemmatizaiton & tokenization\n",
    "# clean_product = spacy_lemma_tokenize(clean_product)\n",
    "# clean_product.to_csv('cleaned_product_data.csv')\n",
    "\n",
    "# use above function to obtain this cleaned data (about 30min to run)\n",
    "clean_product = pd.read_csv('data/cleaned_product_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d2d8a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a844b793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:26.014875Z",
     "start_time": "2021-05-11T18:31:09.894091Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clothing_category feature creation\n",
    "features = pd.DataFrame()\n",
    "features['clothing_category'], clothing_vector, features['clothing_subcategory'], subcategory_vector \\\n",
    "                                = create_clothing_category_feature(clean_product, combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a4ed6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:29.066949Z",
     "start_time": "2021-05-11T18:33:26.016772Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gender feature creation\n",
    "features['gender'], gender_vector = create_gender_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f9ca49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:31.636078Z",
     "start_time": "2021-05-11T18:33:29.068650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# material feature creation\n",
    "features['material'], material_vector = create_material_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9514ccf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:43.332857Z",
     "start_time": "2021-05-11T18:33:31.637706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# material feature creation\n",
    "features['percent_material'], percent_material_vector = create_percent_material_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084afca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:44.249625Z",
     "start_time": "2021-05-11T18:33:43.334508Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# size feature creation \n",
    "features['size'], size_vector = create_sizes_features(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d9f2430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:33:56.151905Z",
     "start_time": "2021-05-11T18:33:44.251268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# color feature creation\n",
    "features['color'], color_vector = create_color_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f914f6de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:01.917730Z",
     "start_time": "2021-05-11T18:33:56.153281Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# location feature creation\n",
    "features['location'], location_vector = create_location_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a280baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:42.203255Z",
     "start_time": "2021-05-11T18:34:01.919344Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tags feature creation\n",
    "features['tags'], tags_vector = create_tags_feature(clean_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ee6aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:46.339341Z",
     "start_time": "2021-05-11T18:34:42.204689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61355, 540)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all features\n",
    "feature_vectors = pd.concat([clothing_vector, subcategory_vector, gender_vector, material_vector, \n",
    "                             percent_material_vector, size_vector, color_vector, location_vector, \n",
    "                             tags_vector], axis=1)\n",
    "\n",
    "# create a final feature for all product that does not belong to any category\n",
    "index = features.loc[features['clothing_category']=='',].index\n",
    "feature_vectors.loc[index, 'uncategorized'] = 1\n",
    "feature_vectors['uncategorized'] =  feature_vectors['uncategorized'].fillna(int(0))\n",
    "\n",
    "# save results\n",
    "features['product_id'] = product['product_id']\n",
    "features[['product_id', 'clothing_category', 'clothing_subcategory']].to_csv('feature_columns.csv')\n",
    "feature_vectors.to_csv('data/feature_vectors.csv')\n",
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73920a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a7b4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1712f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:50.166642Z",
     "start_time": "2021-05-11T18:34:46.346759Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load functions\n",
    "# basic functional packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from numpy.random import seed\n",
    "from collections import Counter\n",
    "\n",
    "# modeling packages\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# nltk package\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tensorflow Keras Toolkit\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# set random seed\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2caf4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed3291cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:50.179246Z",
     "start_time": "2021-05-11T18:34:50.169207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# logistic regression with manual cross validation\n",
    "def make_logreg_function(X, y, cv=10, random_seed=42, max_iter=100, test_size=0.2):\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    accuracy_result = []\n",
    "    np.random.seed(random_seed)\n",
    "    i=1\n",
    "    for iters in np.random.choice(range(0,200,1), size=cv, replace=False):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=test_size,\n",
    "                                                            random_state=iters,\n",
    "                                                            stratify=y)\n",
    "        # transform train test\n",
    "        X_train_std = vectorizer.fit_transform(X_train)\n",
    "        X_test_std = vectorizer.transform(X_test)\n",
    "\n",
    "        # logistic regression model\n",
    "        lr = LogisticRegression(max_iter=max_iter, multi_class='auto')\n",
    "        lr.fit(X_train_std, y_train)\n",
    "\n",
    "        # obtain accuracy\n",
    "        accuracy_result.append(lr.score(X_test_std, y_test))\n",
    "        print(f'Iteration {i} Accuracy: {lr.score(X_test_std, y_test)}');i+=1\n",
    "\n",
    "    print(f'Average Accuracy over 10-Fold Cross Validation: {np.mean(accuracy_result)}')\n",
    "    \n",
    "    return accuracy_result\n",
    "\n",
    "# logistic regression with custom X and manual cross validation\n",
    "def make_custom_logreg_function(X, y, cv=10, random_seed=42, max_iter=100, test_size=0.2):\n",
    "    accuracy_result = []\n",
    "    np.random.seed(random_seed)\n",
    "    i=1\n",
    "    for iters in np.random.choice(range(0,200,1), size=cv, replace=False):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=test_size,\n",
    "                                                            random_state=iters,\n",
    "                                                            stratify=y)\n",
    "        # save index\n",
    "        train_index = X_train.index.to_list()\n",
    "        test_index = X_test.index.to_list()\n",
    "\n",
    "        # transform train test\n",
    "        X_train_std = vectorizer.fit_transform(X_train)\n",
    "        X_test_std = vectorizer.transform(X_test)\n",
    "\n",
    "        # create train test features\n",
    "        feature_train = feature_vectors.loc[train_index]\n",
    "        feature_test = feature_vectors.loc[test_index]\n",
    "\n",
    "        # restore train test table\n",
    "        X_train_table = pd.DataFrame(X_train_std.toarray(), columns=vectorizer.get_feature_names())\n",
    "        X_test_table = pd.DataFrame(X_test_std.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "        # add back index & correct index\n",
    "        X_train_table['index'] = train_index\n",
    "        X_test_table['index'] = test_index\n",
    "        X_train_table.set_index('index', inplace=True)\n",
    "        X_test_table.set_index('index', inplace=True)\n",
    "\n",
    "        X_train_final = pd.concat([X_train_table, feature_train], axis=1)\n",
    "        X_test_final = pd.concat([X_test_table, feature_test], axis=1)\n",
    "\n",
    "        # logistic regression model\n",
    "        lr = LogisticRegression(max_iter=max_iter, multi_class='auto')\n",
    "        lr.fit(X_train_final, y_train)\n",
    "\n",
    "        # obtain accuracy\n",
    "        accuracy_result.append(lr.score(X_test_final, y_test))\n",
    "        print(f'Iteration {i} Accuracy: {lr.score(X_test_final, y_test)}');i+=1\n",
    "\n",
    "    print(f'Average Accuracy over 10-Fold Cross Validation: {np.mean(accuracy_result)}')\n",
    "\n",
    "    return accuracy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9309ec1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:50.188705Z",
     "start_time": "2021-05-11T18:34:50.181286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# LSTM multi classificaiton model\n",
    "def make_lstm_classification_model(embedding_matrix, plot=False,class_value=2, weight_shape=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, weight_shape, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(class_value, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# Simple RNN classsificaiton model\n",
    "def make_multi_classification_rnn_model(embedding_matrix, plot=False,class_value=2, weight_shape=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, weight_shape, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(class_value, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc474642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:50.193732Z",
     "start_time": "2021-05-11T18:34:50.190765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  Glove vectors\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('data/glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743718d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression ( average 93.8% accuracy on 20% validaiton data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78f8ad32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:53.129456Z",
     "start_time": "2021-05-11T18:34:50.195249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('data/cleaned_product_data.csv', index_col=0)\n",
    "feature_vectors = pd.read_csv('data/feature_vectors.csv', index_col=0)\n",
    "df.drop(columns=['brand_category', 'name', 'details', 'description','product_id','product_info',\n",
    "                    'brand_description', 'brand_name', 'all_info', 'raw_product_info'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a2824af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:53.174335Z",
     "start_time": "2021-05-11T18:34:53.131256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# encode top 50 brand labels\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "top50_col = list(df['brand'].value_counts()[:50].index)\n",
    "df['brand'] = df['brand'].map(lambda x: x if x in top50_col else 'other')\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df['brand'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c5eb886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:34:53.181249Z",
     "start_time": "2021-05-11T18:34:53.175948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "stopword_list = list(stopwords.words('English'))\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                             stop_words=stopword_list, \n",
    "                             max_features=1000,\n",
    "                             token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d0b76a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:40:35.129847Z",
     "start_time": "2021-05-11T18:34:53.182477Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Accuracy: 0.9290196398011572\n",
      "Iteration 2 Accuracy: 0.9260044006193464\n",
      "Iteration 3 Accuracy: 0.9277972455382609\n",
      "Iteration 4 Accuracy: 0.9237225979952735\n",
      "Iteration 5 Accuracy: 0.932360850786407\n",
      "Iteration 6 Accuracy: 0.9258414147176269\n",
      "Iteration 7 Accuracy: 0.9278787384891207\n",
      "Iteration 8 Accuracy: 0.9268193301279439\n",
      "Iteration 9 Accuracy: 0.9244560345530112\n",
      "Iteration 10 Accuracy: 0.9242930486512917\n",
      "Average Accuracy over 10-Fold Cross Validation: 0.9268193301279439\n",
      "[0.9290196398011572, 0.9260044006193464, 0.9277972455382609, 0.9237225979952735, 0.932360850786407, 0.9258414147176269, 0.9278787384891207, 0.9268193301279439, 0.9244560345530112, 0.9242930486512917]\n"
     ]
    }
   ],
   "source": [
    "# use 10 fold cv to validate - without additional features\n",
    "X = df['clean_product_info']\n",
    "y = df[\"brand\"].values\n",
    "\n",
    "result = make_logreg_function(X, y, cv=10, random_seed=42, max_iter=500, test_size=0.2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "197ab260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:50:15.806965Z",
     "start_time": "2021-05-11T18:40:35.131801Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Accuracy: 0.939858202265504\n",
      "Iteration 2 Accuracy: 0.938554315051748\n",
      "Iteration 3 Accuracy: 0.9390432727569066\n",
      "Iteration 4 Accuracy: 0.9358650476733763\n",
      "Iteration 5 Accuracy: 0.9426289625947356\n",
      "Iteration 6 Accuracy: 0.9379838643957298\n",
      "Iteration 7 Accuracy: 0.9387987939043273\n",
      "Iteration 8 Accuracy: 0.9401841740689431\n",
      "Iteration 9 Accuracy: 0.9360280335750958\n",
      "Iteration 10 Accuracy: 0.9353760899682177\n",
      "Average Accuracy over 10-Fold Cross Validation: 0.9384320756254583\n",
      "[0.939858202265504, 0.938554315051748, 0.9390432727569066, 0.9358650476733763, 0.9426289625947356, 0.9379838643957298, 0.9387987939043273, 0.9401841740689431, 0.9360280335750958, 0.9353760899682177]\n"
     ]
    }
   ],
   "source": [
    "# use 10 fold cv to validate - with additional features\n",
    "result = make_custom_logreg_function(X, y, cv=10, random_seed=42, max_iter=500, test_size=0.2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16df8d6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM Deep Learning Model (over 93.0% accuracy on 20% validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb3d2d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:50:19.332607Z",
     "start_time": "2021-05-11T18:50:15.809667Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(df['clean_product_info'])\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d999449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:50:22.209760Z",
     "start_time": "2021-05-11T18:50:19.334500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# integer encode documents\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "# integer encode the documents\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "encoded_docs = integer_encode_documents(df['clean_product_info'], tokenizer)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c0919d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:52:16.705837Z",
     "start_time": "2021-05-11T18:50:22.211796Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# create custom embeddings\n",
    "embeddings_index = load_glove_vectors()\n",
    "\n",
    "# Glove Vectors\n",
    "# create a weight matrix for words in training docs\n",
    "glove_embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# Spacy Vectors\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "spacy_embedding_matrix = zeros((VOCAB_SIZE, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = nlp(word).vector\n",
    "    if embedding_vector is not None:\n",
    "        spacy_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12ca8a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:04:13.805555Z",
     "start_time": "2021-05-11T18:52:16.707317Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          10678800  \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 51)                3315      \n",
      "=================================================================\n",
      "Total params: 10,726,851\n",
      "Trainable params: 48,051\n",
      "Non-trainable params: 10,678,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1381/1381 [==============================] - 152s 108ms/step - loss: 1.8941 - accuracy: 0.5597 - val_loss: 0.6036 - val_accuracy: 0.8497\n",
      "Epoch 2/5\n",
      "1381/1381 [==============================] - 147s 106ms/step - loss: 0.5074 - accuracy: 0.8754 - val_loss: 0.4052 - val_accuracy: 0.9034\n",
      "Epoch 3/5\n",
      "1381/1381 [==============================] - 142s 103ms/step - loss: 0.3239 - accuracy: 0.9180 - val_loss: 0.3387 - val_accuracy: 0.9142\n",
      "Epoch 4/5\n",
      "1381/1381 [==============================] - 138s 100ms/step - loss: 0.2485 - accuracy: 0.9369 - val_loss: 0.3400 - val_accuracy: 0.9149\n",
      "Epoch 5/5\n",
      "1381/1381 [==============================] - 137s 99ms/step - loss: 0.2051 - accuracy: 0.9457 - val_loss: 0.2896 - val_accuracy: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98df6f90a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit LSTM model\n",
    "lstm_model = make_lstm_classification_model(class_value=51, weight_shape=300, embedding_matrix=spacy_embedding_matrix)\n",
    "lstm_model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "304916cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:04:22.666974Z",
     "start_time": "2021-05-11T19:04:13.807115Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/384 [==============================] - 9s 23ms/step - loss: 0.2855 - accuracy: 0.9302\n",
      "Accuracy: 93.024206\n"
     ]
    }
   ],
   "source": [
    "# check result\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
